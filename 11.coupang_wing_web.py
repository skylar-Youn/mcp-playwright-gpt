#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¿ íŒ¡ í¬ë¡¤ëŸ¬ ì›¹ ë²„ì „
- Flask ì›¹ ì¸í„°í˜ì´ìŠ¤
- ì‹¤ì‹œê°„ ì—ëŸ¬ íŠ¸ë˜í‚¹
- ë¡œê·¸ í‘œì‹œ ê¸°ëŠ¥
"""

from flask import Flask, render_template, request, jsonify, send_file
from flask_socketio import SocketIO, emit
import asyncio
import json
import os
import csv
import traceback
from datetime import datetime
from threading import Thread
import logging
from logging.handlers import RotatingFileHandler

try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

try:
    from playwright_stealth import Stealth
    STEALTH_AVAILABLE = True
except ImportError:
    STEALTH_AVAILABLE = False

# Flask ì•± ì„¤ì •
app = Flask(__name__)
app.config['SECRET_KEY'] = 'coupang-crawler-secret-key'
socketio = SocketIO(app, cors_allowed_origins="*")

# ë¡œê¹… ì„¤ì •
if not os.path.exists('logs'):
    os.makedirs('logs')

file_handler = RotatingFileHandler('logs/coupang_crawler.log', maxBytes=10240000, backupCount=10)
file_handler.setFormatter(logging.Formatter(
    '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
))
file_handler.setLevel(logging.INFO)
app.logger.addHandler(file_handler)
app.logger.setLevel(logging.INFO)
app.logger.info('ì¿ íŒ¡ í¬ë¡¤ëŸ¬ ì›¹ ì‹œì‘')

# ì„¤ì • íŒŒì¼
CONFIG_FILE = 'coupang_wing_config.json'

# ì „ì—­ ë³€ìˆ˜
scraping_results = []
is_scraping = False


def load_config():
    """ì„¤ì • ë¡œë“œ"""
    default_config = {
        'max_results': 20,
        'min_price': 0,
        'max_price': 999999999,
        'exclude_rocket': True,
        'exclude_rocket_direct': True,
        'headless': True,
        'use_proxy': False,
        'proxy_server': '',
        'min_delay': 2.0,
        'max_delay': 5.0,
        'scroll_delay': 1.5,
        'initial_wait': 3.0,
        'use_stealth': True  # ìŠ¤í…”ìŠ¤ ëª¨ë“œ ê¸°ë³¸ê°’
    }

    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return {**default_config, **json.load(f)}
        except Exception as e:
            app.logger.error(f'ì„¤ì • ë¡œë“œ ì˜¤ë¥˜: {str(e)}')
            return default_config
    return default_config


def save_config(config):
    """ì„¤ì • ì €ì¥"""
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        app.logger.error(f'ì„¤ì • ì €ì¥ ì˜¤ë¥˜: {str(e)}')
        return False


def emit_log(level, message):
    """ë¡œê·¸ ì „ì†¡"""
    log_data = {
        'level': level,
        'message': message,
        'timestamp': datetime.now().strftime('%H:%M:%S')
    }
    socketio.emit('log', log_data)

    # íŒŒì¼ ë¡œê·¸ë„ ê¸°ë¡
    if level == 'error':
        app.logger.error(message)
    elif level == 'warning':
        app.logger.warning(message)
    else:
        app.logger.info(message)


async def smooth_scroll(page, start=0, end=None, steps=10):
    """ì‚¬ëŒì²˜ëŸ¼ ë¶€ë“œëŸ½ê²Œ ìŠ¤í¬ë¡¤í•˜ëŠ” í•¨ìˆ˜"""
    import random

    if end is None:
        end = await page.evaluate("() => document.body.scrollHeight")

    step_size = (end - start) / steps
    current = start

    for _ in range(steps):
        # ì•½ê°„ì˜ ëœë¤ì„±ì„ ì¶”ê°€í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤ êµ¬í˜„
        current += step_size + random.uniform(-100, 100)
        current = min(current, end)
        await page.evaluate(f"window.scrollTo(0, {current})")
        await asyncio.sleep(random.uniform(0.3, 0.7))


async def scrape_coupang(search_params):
    """ì¿ íŒ¡ ìŠ¤í¬ë˜í•‘ (ê°œì„ ëœ ë²„ì „ - ë´‡ ê°ì§€ íšŒí”¼)"""
    global scraping_results, is_scraping
    import random

    results = []
    query = search_params['query']
    max_results = search_params.get('max_results', 20)

    emit_log('info', f"'{query}' ê²€ìƒ‰ ì‹œì‘...")

    try:
        async with async_playwright() as p:
            emit_log('info', 'Playwright ì´ˆê¸°í™” ì¤‘...')

            # ë¸Œë¼ìš°ì € ì‹¤í–‰ (ì•ˆí‹°ë´‡ ìš°íšŒ ì„¤ì •)
            try:
                browser = await p.chromium.launch(
                    headless=search_params.get('headless', True),
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox',
                        '--disable-http2',  # HTTP/2 ë¹„í™œì„±í™” (í”„ë¡œí† ì½œ ì˜¤ë¥˜ ë°©ì§€)
                        '--disable-gpu',
                        '--window-size=1920,1080',
                        '--disable-extensions',
                        '--disable-setuid-sandbox',
                        '--disable-web-security',
                        '--disable-features=IsolateOrigins,site-per-process',
                        '--incognito'  # ì‹œí¬ë¦¿ ëª¨ë“œ
                    ]
                )
                emit_log('success', 'ë¸Œë¼ìš°ì € ì‹¤í–‰ ì™„ë£Œ (ì‹œí¬ë¦¿ ëª¨ë“œ, HTTP/2 ë¹„í™œì„±í™”)')
            except Exception as e:
                emit_log('error', f'ë¸Œë¼ìš°ì € ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}')
                raise

            # ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
            context_options = {
                'viewport': {'width': 1920, 'height': 1080},
                'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'locale': 'ko-KR',
                'timezone_id': 'Asia/Seoul'
            }

            # í”„ë¡ì‹œ ì„¤ì •
            if search_params.get('use_proxy') and search_params.get('proxy_server'):
                proxy_server = search_params.get('proxy_server')
                emit_log('info', f'í”„ë¡ì‹œ ì‚¬ìš©: {proxy_server}')
                context_options['proxy'] = {'server': proxy_server}

            try:
                context = await browser.new_context(**context_options)
                emit_log('success', 'ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ')
            except Exception as e:
                emit_log('error', f'ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}')
                await browser.close()
                raise

            # ìŠ¤í…”ìŠ¤ ëª¨ë“œ ì ìš© (ë´‡ ê°ì§€ ìš°íšŒ) - contextì— ì ìš©
            if search_params.get('use_stealth', True) and STEALTH_AVAILABLE:
                stealth_config = Stealth(
                    navigator_languages_override=('ko-KR', 'ko'),
                    navigator_user_agent_override=None  # ìë™ ì„¤ì •
                )
                await stealth_config.apply_stealth_async(context)
                emit_log('success', 'ìŠ¤í…”ìŠ¤ ëª¨ë“œ í™œì„±í™” ì™„ë£Œ')
            elif search_params.get('use_stealth', True) and not STEALTH_AVAILABLE:
                emit_log('warning', 'playwright-stealth ë¯¸ì„¤ì¹˜ (ê¸°ë³¸ ëª¨ë“œ)')
            else:
                emit_log('info', 'ìŠ¤í…”ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™” (ì‚¬ìš©ì ì„¤ì •)')

            page = await context.new_page()

            try:
                # ê²€ìƒ‰ URL êµ¬ì„±
                import urllib.parse
                encoded_query = urllib.parse.quote(query)
                search_url = f"https://www.coupang.com/np/search?component=&q={encoded_query}&channel=user"

                emit_log('info', f'ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì¤‘: {query}')

                # ì¿ í‚¤ ì‚­ì œ
                await context.clear_cookies()

                # ì‚¬ëŒì²˜ëŸ¼ ëœë¤í•œ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€
                await asyncio.sleep(random.uniform(2, 4))

                # ì—¬ëŸ¬ ë²ˆ ì‹œë„ (ì¬ì‹œë„ ë¡œì§)
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        await page.goto(
                            search_url,
                            wait_until='domcontentloaded',
                            timeout=60000
                        )
                        emit_log('success', 'ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì™„ë£Œ')
                        break
                    except Exception as e:
                        if attempt < max_retries - 1:
                            emit_log('warning', f'ì ‘ì† ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}), ì¬ì‹œë„ ì¤‘...')
                            await asyncio.sleep(3)
                        else:
                            emit_log('error', f'ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {str(e)}')
                            raise

                # í˜ì´ì§€ ë¡œë“œ í›„ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸°
                await asyncio.sleep(random.uniform(3, 5))

                # í˜ì´ì§€ ë¡œë“œ ìƒíƒœ í™•ì¸
                page_state = await page.evaluate('document.readyState')
                emit_log('info', f'í˜ì´ì§€ ë¡œë“œ ìƒíƒœ: {page_state}')

                # ë´‡ íƒì§€ í™•ì¸
                page_content = await page.content()
                if "ë³´ì•ˆ ê²€ì‚¬ ì¤‘ì…ë‹ˆë‹¤" in page_content or "ìº¡ì°¨" in page_content:
                    emit_log('warning', 'ë´‡ íƒì§€ ê°ì§€ë¨. ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„...')
                    await asyncio.sleep(random.uniform(15, 30))

                # ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤ ë™ì‘
                viewport_height = await page.evaluate("window.innerHeight")
                total_height = await page.evaluate("document.body.scrollHeight")

                emit_log('info', 'ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤ ì‹œì‘...')

                # ì¤‘ê°„ ì¤‘ê°„ ë©ˆì¶”ë©´ì„œ ìŠ¤í¬ë¡¤
                current_position = 0
                while current_position < total_height:
                    # ë‹¤ìŒ ìŠ¤í¬ë¡¤ ìœ„ì¹˜ ê³„ì‚° (ëœë¤ì„± ì¶”ê°€)
                    scroll_amount = random.randint(300, 700)
                    next_position = min(current_position + scroll_amount, total_height)

                    # ë¶€ë“œëŸ½ê²Œ ìŠ¤í¬ë¡¤
                    await smooth_scroll(page, current_position, next_position)

                    # ìŠ¤í¬ë¡¤ í›„ ì ì‹œ ëŒ€ê¸° (ì»¨í…ì¸  ë¡œë”© ëŒ€ê¸°)
                    await asyncio.sleep(random.uniform(0.5, 1.5))

                    # ê°€ë” ìœ„ë¡œ ì‚´ì§ ìŠ¤í¬ë¡¤ (ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê²Œ)
                    if random.random() < 0.2:  # 20% í™•ë¥ 
                        back_scroll = random.randint(100, 300)
                        await page.evaluate(f"window.scrollBy(0, -{back_scroll})")
                        await asyncio.sleep(random.uniform(0.3, 0.7))

                    current_position = next_position

                emit_log('success', 'ìŠ¤í¬ë¡¤ ì™„ë£Œ')

                # ìš”ì†Œ ì°¾ê¸° ì „ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ê¸°
                await asyncio.sleep(random.uniform(1, 2))

                # ì œí’ˆ ëª©ë¡ ëŒ€ê¸°
                emit_log('info', 'ì œí’ˆ ëª©ë¡ ë¡œë“œ ëŒ€ê¸° ì¤‘...')
                try:
                    await page.wait_for_selector('li.search-product', timeout=30000)
                except:
                    emit_log('warning', 'ìƒí’ˆ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                    await browser.close()
                    return results

                # ì œí’ˆ ìˆ˜ì§‘
                emit_log('info', 'ì œí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...')
                products = await page.query_selector_all('li.search-product')
                emit_log('info', f'{len(products)}ê°œ ì œí’ˆ ë°œê²¬')

                if not products:
                    emit_log('warning', 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤')
                    await browser.close()
                    return results

                # ì œí’ˆ ì²˜ë¦¬ (ê´‘ê³  ì œì™¸)
                non_ad_rank = 0
                ad_count = 0

                for idx, product in enumerate(products):
                    try:
                        # ê´‘ê³  ìƒí’ˆ ì²´í¬
                        class_attr = await product.get_attribute('class')
                        is_ad = 'search-product__ad' in (class_attr or '')

                        if is_ad:
                            ad_count += 1
                            emit_log('info', f'ê´‘ê³  ìƒí’ˆ ë°œê²¬: {ad_count}ë²ˆì§¸ ê´‘ê³ ')
                            continue

                        non_ad_rank += 1

                        # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
                        name_elem = await product.query_selector('.name')
                        name = await name_elem.inner_text() if name_elem else "ì œëª© ì—†ìŒ"

                        price_elem = await product.query_selector('.price-value')
                        price_text = await price_elem.inner_text() if price_elem else "0"
                        price = int(price_text.replace(',', '').replace('ì›', '').strip())

                        # ê°€ê²© í•„í„°ë§
                        min_price = search_params.get('min_price', 0)
                        max_price = search_params.get('max_price', 999999999)
                        if price < min_price or price > max_price:
                            continue

                        # í•„í„°ë§
                        is_rocket = await product.query_selector('.badge.rocket') is not None
                        is_rocket_direct = await product.query_selector('.badge.rocket-direct') is not None
                        is_rocket_global = await product.query_selector('.badge.rocket-global') is not None
                        is_rocket_fresh = await product.query_selector('.badge.rocket-fresh') is not None

                        if search_params.get('exclude_rocket', True) and is_rocket:
                            continue
                        if search_params.get('exclude_rocket_direct', True) and (is_rocket_direct or is_rocket_global or is_rocket_fresh):
                            continue

                        # URL
                        link_elem = await product.query_selector('a.search-product-link')
                        product_url = ""
                        if link_elem:
                            href = await link_elem.get_attribute('href')
                            if href:
                                product_url = f"https://www.coupang.com{href}" if href.startswith('/') else href

                        # í‰ì 
                        rating_elem = await product.query_selector('.rating')
                        rating = await rating_elem.inner_text() if rating_elem else "N/A"

                        # ë¦¬ë·° ìˆ˜
                        review_elem = await product.query_selector('.rating-total-count')
                        review_count = await review_elem.inner_text() if review_elem else "0"

                        # íŒë§¤ì ì •ë³´
                        seller_type = "ì¼ë°˜ íŒë§¤ì"
                        if is_rocket:
                            seller_type = "ë¡œì¼“ë°°ì†¡"
                        elif is_rocket_direct:
                            seller_type = "ë¡œì¼“ì§êµ¬"
                        elif is_rocket_global:
                            seller_type = "ë¡œì¼“ê¸€ë¡œë²Œ"
                        elif is_rocket_fresh:
                            seller_type = "ë¡œì¼“í”„ë ˆì‹œ"

                        results.append({
                            'rank': non_ad_rank,  # ê´‘ê³  ì œì™¸ ìˆœìœ„
                            'name': name.strip(),
                            'price': price,
                            'seller_type': seller_type,
                            'rating': rating.strip(),
                            'review_count': review_count.strip(),
                            'url': product_url
                        })

                        emit_log('info', f'ì²˜ë¦¬ ì¤‘... {len(results)}ê°œ ì œí’ˆ ë°œê²¬ (ê´‘ê³  ì œì™¸ ìˆœìœ„: {non_ad_rank})')

                        # ì‹¤ì‹œê°„ ê²°ê³¼ ì „ì†¡
                        socketio.emit('result_update', {
                            'count': len(results),
                            'latest': results[-1]
                        })

                        if len(results) >= max_results:
                            break

                    except Exception as e:
                        emit_log('warning', f'ì œí’ˆ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}')
                        continue

                # ê°€ê²© ìˆœ ì •ë ¬
                results.sort(key=lambda x: x['price'])
                emit_log('success', f'ê²€ìƒ‰ ì™„ë£Œ! ì´ {len(results)}ê°œ ì œí’ˆ ìˆ˜ì§‘ (ê´‘ê³  {ad_count}ê°œ ì œì™¸)')

            except Exception as e:
                emit_log('error', f'í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\n{traceback.format_exc()}')
                raise
            finally:
                await browser.close()
                emit_log('info', 'ë¸Œë¼ìš°ì € ì¢…ë£Œ')

    except Exception as e:
        emit_log('error', f'ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}\n{traceback.format_exc()}')
        raise

    return results


def run_scraper(search_params):
    """ìŠ¤í¬ë˜í¼ ì‹¤í–‰ (ë³„ë„ ìŠ¤ë ˆë“œ)"""
    global scraping_results, is_scraping

    try:
        is_scraping = True
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        scraping_results = loop.run_until_complete(scrape_coupang(search_params))
        loop.close()

        # ì™„ë£Œ ì•Œë¦¼
        socketio.emit('scraping_complete', {
            'success': True,
            'count': len(scraping_results),
            'results': scraping_results
        })

    except Exception as e:
        socketio.emit('scraping_complete', {
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        })
    finally:
        is_scraping = False


@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    config = load_config()
    return render_template('index.html', config=config, playwright_available=PLAYWRIGHT_AVAILABLE)


@app.route('/api/config', methods=['GET', 'POST'])
def api_config():
    """ì„¤ì • API"""
    if request.method == 'GET':
        return jsonify(load_config())
    else:
        config = request.json
        if save_config(config):
            return jsonify({'success': True})
        else:
            return jsonify({'success': False, 'error': 'ì„¤ì • ì €ì¥ ì‹¤íŒ¨'}), 500


@app.route('/api/search', methods=['POST'])
def api_search():
    """ê²€ìƒ‰ API"""
    global is_scraping

    if is_scraping:
        return jsonify({'success': False, 'error': 'ì´ë¯¸ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤'}), 400

    if not PLAYWRIGHT_AVAILABLE:
        return jsonify({'success': False, 'error': 'Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}), 500

    search_params = request.json

    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
    thread = Thread(target=run_scraper, args=(search_params,))
    thread.daemon = True
    thread.start()

    return jsonify({'success': True, 'message': 'ê²€ìƒ‰ ì‹œì‘'})


@app.route('/api/results', methods=['GET'])
def api_results():
    """ê²°ê³¼ API"""
    return jsonify(scraping_results)


@app.route('/api/export', methods=['POST'])
def api_export():
    """CSV ë‚´ë³´ë‚´ê¸°"""
    if not scraping_results:
        return jsonify({'success': False, 'error': 'ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤'}), 400

    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'coupang_results_{timestamp}.csv'

        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=['name', 'price', 'seller_type', 'rating', 'review_count', 'url'])
            writer.writeheader()
            writer.writerows(scraping_results)

        return send_file(filename, as_attachment=True)
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/logs', methods=['GET'])
def api_logs():
    """ë¡œê·¸ ì¡°íšŒ"""
    try:
        with open('logs/coupang_crawler.log', 'r', encoding='utf-8') as f:
            logs = f.readlines()[-100:]  # ìµœê·¼ 100ì¤„
        return jsonify({'success': True, 'logs': logs})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@socketio.on('connect')
def handle_connect():
    """WebSocket ì—°ê²°"""
    emit_log('info', 'í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨')


@socketio.on('disconnect')
def handle_disconnect():
    """WebSocket ì—°ê²° í•´ì œ"""
    app.logger.info('í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œ')


if __name__ == '__main__':
    print('=' * 60)
    print('ğŸš€ ì¿ íŒ¡ í¬ë¡¤ëŸ¬ ì›¹ ë²„ì „ ì‹œì‘')
    print('=' * 60)
    print(f'ğŸ“¡ ì„œë²„ ì£¼ì†Œ: http://localhost:5000')
    print(f'ğŸ“Š Playwright: {"âœ… ì„¤ì¹˜ë¨" if PLAYWRIGHT_AVAILABLE else "âŒ ë¯¸ì„¤ì¹˜"}')
    print('=' * 60)
    print('\në¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ì„ ì—´ì–´ì£¼ì„¸ìš”\n')

    socketio.run(app, host='0.0.0.0', port=5000, debug=True, allow_unsafe_werkzeug=True)
