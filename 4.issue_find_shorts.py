#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from playwright.sync_api import sync_playwright
import json
import pandas as pd
from datetime import datetime
import re


def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬ - íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì •ë¦¬"""
    if not text:
        return ""
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def is_shorts_suitable(title, content=""):
    """
    ì‡¼ì¸  ì œì‘ì— ì í•©í•œ ì£¼ì œì¸ì§€ íŒë‹¨

    ì í•© ê¸°ì¤€:
    - í¥ë¯¸ë¡œìš´ ì´ìŠˆ (ë…¼ë€, í™”ì œ)
    - ì‹œê°ì  í‘œí˜„ ê°€ëŠ¥ (ì‚¬ì§„/ì˜ìƒ ê´€ë ¨)
    - ê°ì •ì  ë°˜ì‘ ìœ ë°œ (ì›ƒê¹€, ê°ë™, ë†€ë¼ì›€)
    - ì •ë³´ì„± (ê¿€íŒ, ì‹ ê¸°í•œ ì‚¬ì‹¤)
    """

    # ì œì™¸í•  í‚¤ì›Œë“œ (ê´‘ê³ ì„±, ìŠ¤íŒ¸ì„±, ë¯¼ê°í•œ ì •ì¹˜ ë“±)
    exclude_keywords = [
        'ê´‘ê³ ', 'í™ë³´', 'êµ¬ë§¤', 'íŒë§¤', 'ì•Œë°”', 'ëª¨ì§‘',
        'í˜ì˜¤', 'ë¹„í•˜', 'ìš•ì„¤', 'ì„ ì •ì '
    ]

    # í¬í•¨í•˜ë©´ ì¢‹ì€ í‚¤ì›Œë“œ (ì‡¼ì¸  ì í•©)
    include_keywords = [
        'ì°', 'í›„ê¸°', 'ë ˆì „ë“œ', 'ì‹¤í™”', 'ã„·ã„·', 'ã…‹ã…‹',
        'ë†€ë¼ìš´', 'ì‹ ê¸°í•œ', 'ëŒ€ë°•', 'ì¶©ê²©', 'í™”ì œ',
        'ìµœê·¼', 'ìš”ì¦˜', 'ê·¼í™©', 'TMI', 'íŒ', 'ê¿€íŒ',
        'ë°˜ì‘', 'ì¸ê¸°', 'HOT', 'ë² ìŠ¤íŠ¸', 'ê°œë…ê¸€',
        'ì‚¬ì§„', 'ì˜ìƒ', 'gif', 'ì›€ì§¤'
    ]

    title_lower = title.lower()

    # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
    for keyword in exclude_keywords:
        if keyword in title:
            return False

    # í¬í•¨ í‚¤ì›Œë“œ ì²´í¬ (ê°€ì¤‘ì¹˜)
    score = 0
    for keyword in include_keywords:
        if keyword in title or keyword in title_lower:
            score += 1

    # ì´ëª¨í‹°ì½˜ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìê°€ ë§ìœ¼ë©´ ê°ì • í‘œí˜„ì´ ìˆë‹¤ê³  íŒë‹¨
    emoji_count = len(re.findall(r'[ã…‹ã…ã„·ã„¹ã…‡!?]', title))
    if emoji_count >= 2:
        score += 1

    # ì œëª© ê¸¸ì´ê°€ ì ë‹¹í•˜ë©´ ê°€ì‚°ì 
    if 10 <= len(title) <= 50:
        score += 1

    return score >= 2


def scrape_dcinside():
    """
    DCì¸ì‚¬ì´ë“œ ê°œë…ê¸€/ì‹¤ë²  ìŠ¤í¬ë˜í•‘
    """
    print("\nğŸ” DCì¸ì‚¬ì´ë“œ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        issues = []

        try:
            # ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            page.goto('https://gall.dcinside.com/', wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(2000)

            # ê°œë…ê¸€ ì œëª© ì¶”ì¶œ
            titles = page.locator('strong.tit').all()

            for idx, title_elem in enumerate(titles[:30]):  # ìµœëŒ€ 30ê°œ
                try:
                    title = clean_text(title_elem.inner_text())

                    if not title or len(title) < 5:
                        continue

                    # ë¶€ëª¨ ìš”ì†Œì—ì„œ ê°¤ëŸ¬ë¦¬ëª…ê³¼ ë‚ ì§œ ì°¾ê¸°
                    parent = title_elem.locator('xpath=../..').first

                    gallery = ""
                    date = ""

                    # info divì—ì„œ ê°¤ëŸ¬ë¦¬ëª…ê³¼ ë‚ ì§œ ì¶”ì¶œ
                    info_divs = parent.locator('.info span').all()
                    if len(info_divs) >= 2:
                        gallery = clean_text(info_divs[0].inner_text())
                        date = clean_text(info_divs[1].inner_text())

                    # ë§í¬ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                    link = ""

                    # ë°©ë²• 1: titleì„ í¬í•¨í•œ a íƒœê·¸ ì°¾ê¸°
                    title_parent_link = title_elem.locator('xpath=ancestor::a').first
                    if title_parent_link.count() > 0:
                        href = title_parent_link.get_attribute('href')
                        if href:
                            if href.startswith('http'):
                                link = href
                            elif href.startswith('/'):
                                link = 'https://gall.dcinside.com' + href

                    # ë°©ë²• 2: parentì˜ a íƒœê·¸ ì°¾ê¸°
                    if not link:
                        link_elem = parent.locator('a').first
                        if link_elem.count() > 0:
                            href = link_elem.get_attribute('href')
                            if href:
                                if href.startswith('http'):
                                    link = href
                                elif href.startswith('/'):
                                    link = 'https://gall.dcinside.com' + href

                    issue = {
                        'platform': 'DCì¸ì‚¬ì´ë“œ',
                        'category': gallery,
                        'title': title,
                        'date': date,
                        'link': link,
                        'shorts_suitable': is_shorts_suitable(title),
                        'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    issues.append(issue)

                except Exception as e:
                    print(f"  âš ï¸  ê°œë³„ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            print(f"âœ… DCì¸ì‚¬ì´ë“œ: {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")

        except Exception as e:
            print(f"âŒ DCì¸ì‚¬ì´ë“œ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

        finally:
            browser.close()

        return issues


def scrape_fmkorea():
    """
    FM Korea ë² ìŠ¤íŠ¸ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘
    """
    print("\nğŸ” FM Korea ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        issues = []

        try:
            # ë² ìŠ¤íŠ¸ í˜ì´ì§€ ì ‘ì†
            page.goto('https://www.fmkorea.com/best', wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(2000)

            # ê²Œì‹œë¬¼ ëª©ë¡ ì¶”ì¶œ
            articles = page.locator('.li_best_wrapper, .fm_best_widget li, article').all()

            for idx, article in enumerate(articles[:30]):  # ìµœëŒ€ 30ê°œ
                try:
                    # ì œëª© ë° ë§í¬ ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                    title = ""
                    link = ""
                    title_selectors = [
                        '.title a',
                        'a.hx',
                        '.hotdeal_title a',
                        'a[href*="/board/"]',
                        'a[href*="/hot/"]'
                    ]

                    title_link_elem = None
                    for selector in title_selectors:
                        elem = article.locator(selector).first
                        if elem.count() > 0:
                            title = clean_text(elem.inner_text())
                            if title and len(title) >= 5:
                                title_link_elem = elem
                                break

                    if not title or len(title) < 5:
                        continue

                    # ì œëª© ìš”ì†Œì—ì„œ ë§í¬ ì¶”ì¶œ
                    if title_link_elem:
                        href = title_link_elem.get_attribute('href')
                        if href:
                            if href.startswith('http'):
                                link = href
                            elif href.startswith('/'):
                                link = 'https://www.fmkorea.com' + href

                    # ë§í¬ê°€ ì—†ìœ¼ë©´ articleì˜ ì²« ë²ˆì§¸ a íƒœê·¸ì—ì„œ ì¶”ì¶œ
                    if not link:
                        link_elem = article.locator('a').first
                        if link_elem.count() > 0:
                            href = link_elem.get_attribute('href')
                            if href:
                                if href.startswith('http'):
                                    link = href
                                elif href.startswith('/'):
                                    link = 'https://www.fmkorea.com' + href

                    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                    category = "ë² ìŠ¤íŠ¸"
                    category_elem = article.locator('.category, .category_wrapper').first
                    if category_elem.count() > 0:
                        category = clean_text(category_elem.inner_text())

                    # ë‚ ì§œ ì¶”ì¶œ
                    date = ""
                    date_elem = article.locator('.date, .time, .regdate').first
                    if date_elem.count() > 0:
                        date = clean_text(date_elem.inner_text())

                    issue = {
                        'platform': 'FM Korea',
                        'category': category,
                        'title': title,
                        'date': date,
                        'link': link,
                        'shorts_suitable': is_shorts_suitable(title),
                        'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    issues.append(issue)

                except Exception as e:
                    print(f"  âš ï¸  ê°œë³„ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            print(f"âœ… FM Korea: {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")

        except Exception as e:
            print(f"âŒ FM Korea ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

        finally:
            browser.close()

        return issues


def scrape_natepann():
    """
    ë„¤ì´íŠ¸ íŒ ë² ìŠ¤íŠ¸ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘
    """
    print("\nğŸ” ë„¤ì´íŠ¸ íŒ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        issues = []

        try:
            # ë² ìŠ¤íŠ¸ í˜ì´ì§€ ì ‘ì†
            page.goto('https://pann.nate.com/talk', wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(2000)

            # ê²Œì‹œë¬¼ ëª©ë¡ ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            articles = page.locator('.item_best, .best_item, .list_type li, article, .board_list li').all()

            for idx, article in enumerate(articles[:30]):  # ìµœëŒ€ 30ê°œ
                try:
                    # ì œëª© ë° ë§í¬ ì¶”ì¶œ
                    title = ""
                    link = ""
                    title_selectors = [
                        '.tit a',
                        '.subject a',
                        'a.title',
                        'a[href*="/talk/"]',
                        'h4 a',
                        'strong a'
                    ]

                    title_link_elem = None
                    for selector in title_selectors:
                        elem = article.locator(selector).first
                        if elem.count() > 0:
                            title = clean_text(elem.inner_text())
                            if title and len(title) >= 5:
                                title_link_elem = elem
                                break

                    if not title or len(title) < 5:
                        continue

                    # ì œëª© ìš”ì†Œì—ì„œ ë§í¬ ì¶”ì¶œ
                    if title_link_elem:
                        href = title_link_elem.get_attribute('href')
                        if href:
                            if href.startswith('http'):
                                link = href
                            elif href.startswith('/'):
                                link = 'https://pann.nate.com' + href

                    # ë§í¬ê°€ ì—†ìœ¼ë©´ articleì˜ ì²« ë²ˆì§¸ a íƒœê·¸ì—ì„œ ì¶”ì¶œ
                    if not link:
                        link_elem = article.locator('a').first
                        if link_elem.count() > 0:
                            href = link_elem.get_attribute('href')
                            if href:
                                if href.startswith('http'):
                                    link = href
                                elif href.startswith('/'):
                                    link = 'https://pann.nate.com' + href

                    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                    category = "í† í¬"
                    category_elem = article.locator('.category, .cate, .badge').first
                    if category_elem.count() > 0:
                        category = clean_text(category_elem.inner_text())

                    # ë‚ ì§œ ì¶”ì¶œ
                    date = ""
                    date_elem = article.locator('.date, .time, .regdate, .write_date').first
                    if date_elem.count() > 0:
                        date = clean_text(date_elem.inner_text())

                    issue = {
                        'platform': 'ë„¤ì´íŠ¸ íŒ',
                        'category': category,
                        'title': title,
                        'date': date,
                        'link': link,
                        'shorts_suitable': is_shorts_suitable(title),
                        'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    issues.append(issue)

                except Exception as e:
                    print(f"  âš ï¸  ê°œë³„ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            print(f"âœ… ë„¤ì´íŠ¸ íŒ: {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")

        except Exception as e:
            print(f"âŒ ë„¤ì´íŠ¸ íŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

        finally:
            browser.close()

        return issues


def scrape_naver_bboom():
    """
    ë„¤ì´ë²„ ë¶ì—… ì¸ê¸° ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘
    """
    print("\nğŸ” ë„¤ì´ë²„ ë¶ì—… ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        issues = []

        try:
            # ë¶ì—… ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            page.goto('https://m.bboom.naver.com/', wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(2000)

            # ê²Œì‹œë¬¼ ëª©ë¡ ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            articles = page.locator('.post_item, .item, .list_item, article, li[class*="post"]').all()

            for idx, article in enumerate(articles[:30]):  # ìµœëŒ€ 30ê°œ
                try:
                    # ì œëª© ë° ë§í¬ ì¶”ì¶œ
                    title = ""
                    link = ""
                    title_selectors = [
                        '.post_title a',
                        '.title a',
                        'a.title',
                        '.subject a',
                        'a[href*="/post/"]',
                        'h3 a',
                        'h4 a',
                        'strong a'
                    ]

                    title_link_elem = None
                    for selector in title_selectors:
                        elem = article.locator(selector).first
                        if elem.count() > 0:
                            title = clean_text(elem.inner_text())
                            if title and len(title) >= 5:
                                title_link_elem = elem
                                break

                    if not title or len(title) < 5:
                        continue

                    # ì œëª© ìš”ì†Œì—ì„œ ë§í¬ ì¶”ì¶œ
                    if title_link_elem:
                        href = title_link_elem.get_attribute('href')
                        if href:
                            if href.startswith('http'):
                                link = href
                            elif href.startswith('/'):
                                link = 'https://m.bboom.naver.com' + href

                    # ë§í¬ê°€ ì—†ìœ¼ë©´ articleì˜ ì²« ë²ˆì§¸ a íƒœê·¸ì—ì„œ ì¶”ì¶œ
                    if not link:
                        link_elem = article.locator('a').first
                        if link_elem.count() > 0:
                            href = link_elem.get_attribute('href')
                            if href:
                                if href.startswith('http'):
                                    link = href
                                elif href.startswith('/'):
                                    link = 'https://m.bboom.naver.com' + href

                    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                    category = "ë¶ì—…"
                    category_elem = article.locator('.category, .cate, .badge, .tag').first
                    if category_elem.count() > 0:
                        category = clean_text(category_elem.inner_text())

                    # ë‚ ì§œ ì¶”ì¶œ
                    date = ""
                    date_elem = article.locator('.date, .time, .regdate, .post_date').first
                    if date_elem.count() > 0:
                        date = clean_text(date_elem.inner_text())

                    issue = {
                        'platform': 'ë„¤ì´ë²„ ë¶ì—…',
                        'category': category,
                        'title': title,
                        'date': date,
                        'link': link,
                        'shorts_suitable': is_shorts_suitable(title),
                        'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    issues.append(issue)

                except Exception as e:
                    print(f"  âš ï¸  ê°œë³„ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            print(f"âœ… ë„¤ì´ë²„ ë¶ì—…: {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")

        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ ë¶ì—… ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

        finally:
            browser.close()

        return issues


def scrape_reddit():
    """
    Reddit ì¸ê¸° ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘ (r/popular)
    """
    print("\nğŸ” Reddit ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        issues = []

        try:
            # Reddit ì¸ê¸° í˜ì´ì§€ ì ‘ì†
            page.goto('https://www.reddit.com/r/popular/', wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(3000)

            # ê²Œì‹œë¬¼ ëª©ë¡ ì¶”ì¶œ
            articles = page.locator('shreddit-post, [data-testid="post-container"], article').all()

            for idx, article in enumerate(articles[:30]):  # ìµœëŒ€ 30ê°œ
                try:
                    # ì œëª© ì¶”ì¶œ
                    title = ""
                    title_selectors = [
                        '[slot="title"]',
                        'h3',
                        '[data-testid="post-title"]',
                        'a[slot="full-post-link"]',
                        '.title'
                    ]

                    title_elem = None
                    for selector in title_selectors:
                        elem = article.locator(selector).first
                        if elem.count() > 0:
                            title = clean_text(elem.inner_text())
                            if title and len(title) >= 5:
                                title_elem = elem
                                break

                    if not title or len(title) < 5:
                        continue

                    # ë§í¬ ì¶”ì¶œ
                    link = ""
                    permalink = article.get_attribute('permalink')
                    if permalink:
                        link = 'https://www.reddit.com' + permalink
                    else:
                        # ëŒ€ì²´ ë°©ë²•: a íƒœê·¸ì—ì„œ ì¶”ì¶œ
                        link_elem = article.locator('a[href*="/r/"]').first
                        if link_elem.count() > 0:
                            href = link_elem.get_attribute('href')
                            if href:
                                if href.startswith('http'):
                                    link = href
                                elif href.startswith('/'):
                                    link = 'https://www.reddit.com' + href

                    # ì„œë¸Œë ˆë”§(ì¹´í…Œê³ ë¦¬) ì¶”ì¶œ
                    category = "popular"
                    subreddit_selectors = [
                        '[data-testid="subreddit-name"]',
                        'a[href*="/r/"][href*="comments"]',
                        'shreddit-subreddit-text'
                    ]

                    for selector in subreddit_selectors:
                        subreddit_elem = article.locator(selector).first
                        if subreddit_elem.count() > 0:
                            category = clean_text(subreddit_elem.inner_text())
                            if category:
                                break

                    # ë‚ ì§œ/ì‹œê°„ ì¶”ì¶œ
                    date = ""
                    date_selectors = [
                        'faceplate-timeago',
                        '[data-testid="post-timestamp"]',
                        'time'
                    ]

                    for selector in date_selectors:
                        date_elem = article.locator(selector).first
                        if date_elem.count() > 0:
                            date = clean_text(date_elem.inner_text())
                            if not date:
                                # datetime ì†ì„± í™•ì¸
                                date = date_elem.get_attribute('datetime')
                            if date:
                                break

                    issue = {
                        'platform': 'Reddit',
                        'category': category,
                        'title': title,
                        'date': date,
                        'link': link,
                        'shorts_suitable': is_shorts_suitable(title),
                        'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }

                    issues.append(issue)

                except Exception as e:
                    print(f"  âš ï¸  ê°œë³„ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            print(f"âœ… Reddit: {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")

        except Exception as e:
            print(f"âŒ Reddit ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

        finally:
            browser.close()

        return issues


def select_platform():
    """
    ì‚¬ìš©ìê°€ í”Œë«í¼ì„ ì„ íƒí•˜ë„ë¡ í•¨
    """
    print("\n" + "="*60)
    print("ğŸ“± ì‡¼ì¸  ì œì‘ìš© ì´ìŠˆ ì°¾ê¸°")
    print("="*60)
    print("\nì»¤ë®¤ë‹ˆí‹° í”Œë«í¼ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. DCì¸ì‚¬ì´ë“œ")
    print("2. FM Korea")
    print("3. ë„¤ì´íŠ¸ íŒ")
    print("4. ë„¤ì´ë²„ ë¶ì—…")
    print("5. Reddit")
    print("6. ì „ì²´ (ëª¨ë“  í”Œë«í¼)")
    print("="*60)

    while True:
        choice = input("\nì„ íƒ (1-6): ").strip()

        if choice == '1':
            return ['dcinside']
        elif choice == '2':
            return ['fmkorea']
        elif choice == '3':
            return ['natepann']
        elif choice == '4':
            return ['naver_bboom']
        elif choice == '5':
            return ['reddit']
        elif choice == '6':
            return ['dcinside', 'fmkorea', 'natepann', 'naver_bboom', 'reddit']
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-6 ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")


def main():
    # í”Œë«í¼ ì„ íƒ
    platforms = select_platform()

    all_issues = []

    # ì„ íƒí•œ í”Œë«í¼ë³„ ìŠ¤í¬ë˜í•‘
    for platform in platforms:
        if platform == 'dcinside':
            issues = scrape_dcinside()
        elif platform == 'fmkorea':
            issues = scrape_fmkorea()
        elif platform == 'natepann':
            issues = scrape_natepann()
        elif platform == 'naver_bboom':
            issues = scrape_naver_bboom()
        elif platform == 'reddit':
            issues = scrape_reddit()
        else:
            continue

        all_issues.extend(issues)

    if not all_issues:
        print("\nâŒ ìˆ˜ì§‘ëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì‡¼ì¸  ì í•©í•œ ì£¼ì œë§Œ í•„í„°ë§
    suitable_issues = [issue for issue in all_issues if issue['shorts_suitable']]

    print(f"\nğŸ“Š ì´ {len(all_issues)}ê°œ ì´ìŠˆ ì¤‘ {len(suitable_issues)}ê°œê°€ ì‡¼ì¸  ì œì‘ì— ì í•©í•©ë‹ˆë‹¤.")

    # ìƒìœ„ 10ê°œ ì„ íƒ (ì‡¼ì¸  ì í•©í•œ ê²ƒ ìš°ì„ )
    top_10_suitable = suitable_issues[:10] if len(suitable_issues) >= 10 else suitable_issues

    # ë¶€ì¡±í•˜ë©´ ì „ì²´ì—ì„œ ì±„ì›€
    if len(top_10_suitable) < 10:
        remaining = [issue for issue in all_issues if not issue['shorts_suitable']]
        needed = 10 - len(top_10_suitable)
        top_10_suitable.extend(remaining[:needed])

    print(f"\nğŸ¯ ì‡¼ì¸  ì œì‘ ì¶”ì²œ ì£¼ì œ TOP {len(top_10_suitable)}:")
    print("="*80)

    for idx, issue in enumerate(top_10_suitable, 1):
        suitable_mark = "â­" if issue['shorts_suitable'] else "  "
        print(f"\n{idx}. {suitable_mark} {issue['category']}")
        print(f"   ğŸ“Œ ì œëª©: {issue['title']}")
        print(f"   ğŸ“° ì¶œì²˜: {issue['platform']}")
        print(f"   ğŸ”— ë§í¬: {issue.get('link', 'ë§í¬ ì—†ìŒ')}")
        if issue.get('date'):
            print(f"   ğŸ“… ë‚ ì§œ: {issue['date']}")

    # ê²°ê³¼ ì €ì¥
    date_str = datetime.now().strftime('%Y%m%d_%H%M%S')

    # JSON ì €ì¥ (ì „ì²´)
    json_filename = f'issues_all_{date_str}.json'
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(all_issues, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ê°€ '{json_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # JSON ì €ì¥ (TOP 10)
    json_top10_filename = f'issues_top10_{date_str}.json'
    with open(json_top10_filename, 'w', encoding='utf-8') as f:
        json.dump(top_10_suitable, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ TOP 10ì´ '{json_top10_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì—‘ì…€ ì €ì¥
    excel_filename = f'issues_{date_str}.xlsx'
    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
        # ì „ì²´ ì´ìŠˆ
        df_all = pd.DataFrame(all_issues)
        cols = ['platform', 'category', 'title', 'date', 'shorts_suitable', 'link', 'scraped_at']
        cols = [c for c in cols if c in df_all.columns]
        df_all = df_all[cols]

        # ì»¬ëŸ¼ëª…ì„ í•œê¸€ë¡œ ë³€ê²½
        df_all = df_all.rename(columns={
            'platform': 'ì¶œì²˜',
            'category': 'ì¹´í…Œê³ ë¦¬',
            'title': 'ì œëª©',
            'date': 'ë‚ ì§œ',
            'shorts_suitable': 'ì‡¼ì¸ ì í•©',
            'link': 'ë§í¬',
            'scraped_at': 'ìˆ˜ì§‘ì‹œê°„'
        })
        df_all.to_excel(writer, sheet_name='ì „ì²´', index=False)

        # TOP 10
        df_top10 = pd.DataFrame(top_10_suitable)
        df_top10 = df_top10[cols]
        df_top10 = df_top10.rename(columns={
            'platform': 'ì¶œì²˜',
            'category': 'ì¹´í…Œê³ ë¦¬',
            'title': 'ì œëª©',
            'date': 'ë‚ ì§œ',
            'shorts_suitable': 'ì‡¼ì¸ ì í•©',
            'link': 'ë§í¬',
            'scraped_at': 'ìˆ˜ì§‘ì‹œê°„'
        })
        df_top10.to_excel(writer, sheet_name='TOP10_ì‡¼ì¸ ì¶”ì²œ', index=False)

        # ì‡¼ì¸  ì í•©ë§Œ
        if suitable_issues:
            df_suitable = pd.DataFrame(suitable_issues)
            df_suitable = df_suitable[cols]
            df_suitable = df_suitable.rename(columns={
                'platform': 'ì¶œì²˜',
                'category': 'ì¹´í…Œê³ ë¦¬',
                'title': 'ì œëª©',
                'date': 'ë‚ ì§œ',
                'shorts_suitable': 'ì‡¼ì¸ ì í•©',
                'link': 'ë§í¬',
                'scraped_at': 'ìˆ˜ì§‘ì‹œê°„'
            })
            df_suitable.to_excel(writer, sheet_name='ì‡¼ì¸ ì í•©', index=False)

    print(f"ğŸ’¾ ì—‘ì…€ ê²°ê³¼ê°€ '{excel_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("\nâœ¨ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
